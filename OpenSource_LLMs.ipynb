{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO1ZKuSYO7v/aXwGEs4nbK2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"9DDGEL7yiRLF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UfqyeeQGVyd7"},"outputs":[],"source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from torch import nn\n","\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","text = \"I love programming with Python!\"\n","\n","inputs = tokenizer(text, return_tensors='pt')\n","\n","outputs = model(**inputs)\n","logits = outputs.logits\n","\n","softmax = nn.Softmax(dim=-1)\n","probabilities = softmax(logits)\n","\n","print(probabilities)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"1FWG2gToiTDq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RdyCr12JiTkK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","model_name = \"t5-small\"\n","tokenizer = T5Tokenizer.from_pretrained(model_name)\n","model = T5ForConditionalGeneration.from_pretrained(model_name)\n","\n","input_text = \"summarize: The quick brown fox jumps over the lazy dog. The dog does not mind and continues to sleep.\"\n","\n","inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n","\n","outputs = model.generate(inputs, max_length=50, num_beams=2, early_stopping=True)\n","\n","summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(\"Summary:\", summary)\n"],"metadata":{"id":"xxl16_zviTm9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","model_name = \"t5-small\"\n","tokenizer = T5Tokenizer.from_pretrained(model_name)\n","model = T5ForConditionalGeneration.from_pretrained(model_name)\n","\n","input_text = \"translate English to French: I love programming.\"\n","\n","inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n","\n","outputs = model.generate(inputs.input_ids, max_length=40, num_beams=4, early_stopping=True)\n","\n","translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(\"Translation:\", translation)\n"],"metadata":{"id":"QeOui6YqiWxa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IvdO1t3Nidn8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gIpYnEWQidrT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","\n","input_text = \"Once upon a time in a faraway land\"\n","input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n","\n","output = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2)\n","\n","generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","print(generated_text)\n"],"metadata":{"id":"tWC44Yxmid1D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","summarizer = pipeline(\"summarization\", model=\"gpt2\")\n","\n","text = \"OpenAI developed GPT, a state-of-the-art language model that excels at text generation...\"\n","\n","summary = summarizer(text, max_length=50, min_length=20, do_sample=False)\n","print(summary[0]['summary_text'])\n"],"metadata":{"id":"IRwMVARkii6C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","qa_pipeline = pipeline(\"question-answering\", model=\"gpt2\")\n","\n","context = \"GPT is a language model developed by OpenAI that generates human-like text.\"\n","question = \"Who developed GPT?\"\n","\n","answer = qa_pipeline(question=question, context=context)\n","print(answer['answer'])\n"],"metadata":{"id":"542eP54iiqoS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1l9OXg5RitcS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-DFVBSgsithF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","model_name = \"tiiuae/falcon-7b-instruct\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","input_text = \"What are the benefits of using Falcon models?\"\n","inputs = tokenizer(input_text, return_tensors=\"pt\")\n","outputs = model.generate(**inputs, max_length=100)\n","print(tokenizer.decode(outputs[0]))\n"],"metadata":{"id":"csrFWi7litj9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HYtpzgFyixey"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iJ3zmN-ckLQK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","model_name = \"meta-llama/Llama-2-13b-hf\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","input_text = \"Explain the concept of artificial intelligence.\"\n","inputs = tokenizer(input_text, return_tensors=\"pt\")\n","outputs = model.generate(**inputs, max_length=100)\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"],"metadata":{"id":"aq3RztpQkLTh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fUwEwfRukM2i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install torch"],"metadata":{"id":"QTeKv3VVkM5k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"Meta/llama-3-70b\")  # Replace with actual model path\n","model = AutoModelForCausalLM.from_pretrained(\"Meta/llama-3-70b\")\n","\n","input_text = \"Hello, how are you?\"\n","inputs = tokenizer(input_text, return_tensors=\"pt\")\n","\n","with torch.no_grad():\n","    outputs = model.generate(inputs['input_ids'], max_length=50)\n","\n","output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(output_text)\n"],"metadata":{"id":"ZlTz5uIakM7l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"mistral-7b\")  # Replace with actual model path\n","model = AutoModelForCausalLM.from_pretrained(\"mistral-7b\")\n","\n","input_text = \"What is the capital of France?\"\n","inputs = tokenizer(input_text, return_tensors=\"pt\")\n","\n","with torch.no_grad():\n","    outputs = model.generate(inputs['input_ids'], max_length=50)\n","\n","output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(output_text)\n"],"metadata":{"id":"uJsYpQQKmUeR"},"execution_count":null,"outputs":[]}]}