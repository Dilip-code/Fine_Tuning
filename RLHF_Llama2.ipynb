{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efNjMrQ5CwDb",
        "outputId": "4475859d-c905-44be-f0ad-9f5393a31e8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "unoconv is already the newest version (0.7-2ubuntu1).\n",
            "libreoffice is already the newest version (1:7.3.7-0ubuntu0.22.04.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Requirement already satisfied: transformers==4.35.2 in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q auto-gptq==0.4.2  tika aspose-words accelerate\n",
        "!apt-get install -y unoconv libreoffice\n",
        "# !pip install \"git+https://github.com/huggingface/transformers.git\"\n",
        "!pip install transformers==4.35.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-Ql04DEDbNu",
        "outputId": "1514c945-ca0f-4437-d5db-038e776559c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, pipeline, logging\n",
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "from tika import parser\n",
        "import re\n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "import time\n",
        "from huggingface_hub import hf_hub_download\n",
        "import aspose.words as aw\n",
        "from io import StringIO\n",
        "from bs4 import BeautifulSoup\n",
        "import ast\n",
        "import itertools\n",
        "import random\n",
        "import subprocess"
      ],
      "metadata": {
        "id": "NUNhe6SMDbQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here, we are using LLaMa-2-13b 4 bit quantized model for question generation\n",
        "\n",
        "local_folder = \"/content/drive/MyDrive/LLaMa-2-13b-chat-GPTQ-4bit\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(local_folder, use_fast=True)\n",
        "model = AutoGPTQForCausalLM.from_quantized(local_folder,\n",
        "        model_basename=\"model\",\n",
        "        use_safetensors=True,\n",
        "        trust_remote_code=True,\n",
        "        device=\"cuda:0\",\n",
        "        use_triton=False,\n",
        "        quantize_config=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipMpalFODbSw",
        "outputId": "e43650c1-bf3a-418f-f774-ee98574458fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:auto_gptq.nn_modules.fused_llama_mlp:skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mcq(Text):\n",
        "    ''' Returns multiple choice questions with corresponding options , answer, page_number and rank\n",
        "\n",
        "    Parameters:\n",
        "    Text(list): contain list of all required pages\n",
        "\n",
        "    Returns:\n",
        "    final mcq (list): contain list of dictionaries (each dictionary has each question, options, answer, page_number)\n",
        "\n",
        "    Sample_output: [{'question': 'Who is the Prime minister of India?',\n",
        "                     'options': ['Pakistan', 'Nepal', 'India', 'China'],\n",
        "                     'answer': 'India',\n",
        "                     'page_no':int}] '''\n",
        "    mcq_by_LLM = []\n",
        "    for index,item in enumerate(Text):\n",
        "        # check page length means text length in particular page\n",
        "        if len(item)>100:\n",
        "\n",
        "            # We will generate all possible mcq from single page in json format\n",
        "            prompt = \"Generate as many as possible hard Multiple choice questions with four options and answer using this text:\"+\" \"+item+'''\\n Give me output in this JSON array format:[{\"question\": string, \"options\":List[string], \"answer\":string}]'''\n",
        "            prompt_template=f'''[INST] <<SYS>>\n",
        "            You are best in generating Multiple choice questions with demanded JSON format.\n",
        "            <</SYS>>\n",
        "            {prompt}[/INST]'''\n",
        "\n",
        "            input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "            output = model.generate(inputs=input_ids, temperature=0.1, top_p=0.9, max_new_tokens=768)\n",
        "            op=tokenizer.decode(output[0])\n",
        "            llm_output=op.split('[/INST]')[1].strip()\n",
        "\n",
        "            # once we have all generated mcq from single page we will rank them as per their quality\n",
        "            prompt = llm_output+\" \\n for above text assign ranking score based on the quality of question to each question in terms of percentage varying from o% to 100%, you can use this text from which questions are generated as a reference, text:\"+\" \"+item+'''\\n Give me output in this JSON array format:[{\"question\": string, \"options\":List[string], \"answer\":string, \"rank\":string}]'''\n",
        "            prompt_template=f'''[INST] <<SYS>>\n",
        "            You are best in ranking Multiple choice questions based on quality of question. Generate output in demanded JSON format.\n",
        "            <</SYS>>\n",
        "            {prompt}[/INST]'''\n",
        "\n",
        "            input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "            output = model.generate(inputs=input_ids, temperature=0.1, top_p=0.9, max_new_tokens=768)\n",
        "            op=tokenizer.decode(output[0])\n",
        "            llm_output_2=op.split('[/INST]')[1].strip()\n",
        "            mcq_by_LLM.append(llm_output_2)\n",
        "\n",
        "        # If page has short text length than will add below strings so that we will get exact page number\n",
        "        else:\n",
        "            mcq_by_LLM.append('Questions are not generated due to shorter length')\n",
        "\n",
        "    def extract_data(entry):\n",
        "        # This function will return dictionary with all required keys\n",
        "        question = entry.get(\"question\", None)\n",
        "        options = entry.get(\"options\", None)\n",
        "        answer = entry.get(\"answer\", None)\n",
        "        rank= entry.get(\"rank\", None)\n",
        "        return {\"question\": question, \"options\": options, \"answer\": answer, \"rank\":rank}\n",
        "\n",
        "    def process_data(data_string):\n",
        "        ''' Returns list with dictionaries as element\n",
        "\n",
        "        Parameters:\n",
        "        data_string(str): raw output from LLM\n",
        "\n",
        "        Returns:\n",
        "        question_dicts(list): contain list of dictionaries (each dictionary has each question, options, answer, rank)\n",
        "        '''\n",
        "        # Enclose keys and values in double quotes\n",
        "        data_string = re.sub(r'([{,])\\s?([a-zA-Z_]+[a-zA-Z0-9_]*)\\s?:', r'\\1\"\\2\":', data_string)\n",
        "        data_string = re.sub(r'\"(.*?)\"', r'\"\\1\"', data_string)\n",
        "\n",
        "        # Add double quotes around rank values\n",
        "        data_string = re.sub(r'\"rank\"\\s?:\\s?(\\d+)%', r'\"rank\":\"\\1%\"', data_string)\n",
        "        data_string = re.sub(r'\"rank\"\\s?:\\s?(\\d+%?)', r'\"rank\":\"\\1\"', data_string)\n",
        "        data_string = re.sub(r'\"rank\"\\s?:\\s?\"?(\\d+)%\"?', r'\"rank\":\"\\1\"', data_string)\n",
        "\n",
        "        data_list = []\n",
        "        # using regex we will find all entries which enclosed in {}\n",
        "        entries = re.findall(r\"{.*?}\", data_string)\n",
        "        for entry in entries:\n",
        "            try:\n",
        "                # this line will convert all dictionary in valid json format\n",
        "                data_list.append(eval(entry))\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "        # once we have all required keys and values for mcq question type we will get in list format\n",
        "        question_dicts = [extract_data(entry) for entry in data_list if all(key in entry and entry[key] for key in [\"question\", \"options\", \"answer\", \"rank\"])]\n",
        "        return question_dicts\n",
        "\n",
        "    # Here, we will take raw output of LLM and then clean it and apply above functions to get final_generated_questions\n",
        "    final_generated_questions = []\n",
        "    for index,data in enumerate(mcq_by_LLM):\n",
        "        llm_output = data.replace('\\\\', '').replace('\\\\n', '')   # replace all \\n\n",
        "        llm_output = ' '.join(llm_output.split())\n",
        "        # Use above process data function to extract dictionary elements from string output of LLM\n",
        "        data_updated=process_data(llm_output)\n",
        "\n",
        "        for item in data_updated:\n",
        "            if file_type == \"mp4\":\n",
        "                item['context']=final[index]\n",
        "            else:\n",
        "                item['context']=None\n",
        "            item['page_no']=int(index+1)\n",
        "            item['statement']=None\n",
        "            item['question_type']=\"mcq\"\n",
        "            if type(item['options']) == str:\n",
        "                item['options'] = [option.capitalize() for option in item['options']]\n",
        "            if type(item['answer']) == str:\n",
        "                item['answer'] = item['answer'].capitalize()\n",
        "            else:\n",
        "                item['answer'] = item['answer']\n",
        "        final_generated_questions.append(data_updated)\n",
        "    final_generated_questions=flatten(final_generated_questions)\n",
        "\n",
        "    # Apply modify_answer_based_on_input function to get cleaned options and answer\n",
        "    final_generated_questions=modify_answer_based_on_input(final_generated_questions)\n",
        "\n",
        "    # Sorting of final MCQ based on rank\n",
        "    final_generated_questions=sorted(final_generated_questions, key=lambda x:(x['rank']), reverse=True)\n",
        "    return final_generated_questions"
      ],
      "metadata": {
        "id": "dwF5mPPzDbWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final = [\"\"\"\n",
        "This story is about Lencho, a dedicated farmer and how he places trust in God to help him out of his\n",
        "misery. Lencho had hoped for a good harvest, but a hail storm destroyed his crops. He was\n",
        "devastated, but he firmly believed that God would help him. He knew how to write, so he wrote a\n",
        "letter to God, asking him to send 100 pesos and posted the letter.\n",
        "The postman noticed the letter and pulled it out of the mailbox. Upon seeing whom it was addressed\n",
        "to, he started laughing loudly. He ran to the postmaster to show him the strange letter. As the\n",
        "postmaster read the contents of the letter, he became very serious. He decided to help Lencho\n",
        "financially by asking for donations from the post office employees. The postmaster himself decided\n",
        "to put a part of his salary into helping Lencho.\n",
        "However, they could only raise 70 pesos and decide to put it in an envelope and sign it off in the\n",
        "name of God. The following Sunday, Lencho visited the post office and asked if there was any letter\n",
        "for him. The postmaster handed him the letter. Lencho did not get surprised seeing the money but\n",
        "got dismayed upon counting it. He was sure that God could not make a mistake, so he took paper\n",
        "and ink, wrote another letter to God, and put it in the mailbox.\n",
        "\"\"\"]"
      ],
      "metadata": {
        "id": "sn1n0TtKEEWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "request=requests.get(\"https://generate-questions.devbyopeneyes.com/api/getFileData/65e84280970073c3d008ab82\")\n",
        "resp = request.json()\n",
        "file_name = (resp[\"data\"][\"file_name\"])\n",
        "_id = (resp[\"data\"][\"_id\"])\n",
        "file_type = (resp[\"data\"][\"file_type\"])\n",
        "user_file_name = (resp[\"data\"][\"file_name\"])\n",
        "type_of_question = (resp[\"data\"][\"type_of_question\"])\n",
        "if file_type == \"mp4\":\n",
        "    pdf_file_path = (resp[\"data\"][\"pdf_file_path\"])\n",
        "    result = (resp[\"data\"][\"pdf_file_result\"])\n",
        "else:\n",
        "    file_path = (resp[\"data\"][\"file_path\"])"
      ],
      "metadata": {
        "id": "aU_7lwGBGVGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converts nested list to single list\n",
        "def flatten(list_of_lists):\n",
        "    return list(itertools.chain.from_iterable(list_of_lists))"
      ],
      "metadata": {
        "id": "GHJq85e6GpkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def final_boolean_statement(true_statement,false_statement):\n",
        "    ''' Returns list with dictionaries (questions) as element\n",
        "\n",
        "    Parameters:\n",
        "    true_statement(list): contain list of dictionaries with true answered statements\n",
        "    false_statement(list): contain list of dictionaries with false answered statements\n",
        "\n",
        "    Returns:\n",
        "    for each page we will take half of true_statement and half of false_statement and combine them\n",
        "    '''\n",
        "    final_list=[]\n",
        "    for list_1,list_2 in zip(true_statement,false_statement):\n",
        "        first_half_list1 = list_1[:len(list_1) // 2]\n",
        "        second_half_list2 = list_2[len(list_2) // 2:]\n",
        "        final_list.append(first_half_list1 + second_half_list2)\n",
        "    final_list=flatten(final_list)\n",
        "    # we are shuffling questions so that we can display random required and additional questions\n",
        "    random.shuffle(final_list)\n",
        "    return final_list\n",
        "\n",
        "def modify_answer_based_on_input(questions_list):\n",
        "    ''' Returns questions list with modified answer and options\n",
        "\n",
        "    Parameters:\n",
        "    questions_list(list): We will check each dictionary's options and answer and modify it.'''\n",
        "    try:\n",
        "        for question in questions_list:\n",
        "            answer = question['answer']\n",
        "            options = question['options']\n",
        "\n",
        "            # This condition will check if answer has onlhy a,b,c or d then will return answer as actual text string\n",
        "            if type(answer)!=bool:\n",
        "                if answer.lower() == 'a':\n",
        "                    question['answer'] = options[0]\n",
        "                elif answer.lower() == 'b':\n",
        "                    question['answer'] = options[1]\n",
        "                elif answer.lower() == 'c':\n",
        "                    question['answer'] = options[2]\n",
        "                elif answer.lower() == 'd':\n",
        "                    question['answer'] = options[3]\n",
        "            # This condition will check if options has A, B, C ,D or 1,2,3,4 in the beginning then it will remove it\n",
        "            if len(options)==2:\n",
        "                for i in range(2):\n",
        "                    options[i] = options[i].replace(f'{chr(65 + i)})', '').replace(f'{chr(65 + i)}.', '').replace(f'{chr(97+ i)}.', '').replace(f'{chr(97 + i)})', '').replace(f'{i + 1})', '').replace(f'{i + 1}.', '').strip()\n",
        "            if len(options)==3:\n",
        "                for i in range(3):\n",
        "                    options[i] = options[i].replace(f'{chr(65 + i)})', '').replace(f'{chr(65 + i)}.', '').replace(f'{chr(97+ i)}.', '').replace(f'{chr(97 + i)})', '').replace(f'{i + 1})', '').replace(f'{i + 1}.', '').strip()\n",
        "            if len(options)==4:\n",
        "                for i in range(4):\n",
        "                    options[i] = options[i].replace(f'{chr(65 + i)})', '').replace(f'{chr(65 + i)}.', '').replace(f'{chr(97+ i)}.', '').replace(f'{chr(97 + i)})', '').replace(f'{i + 1})', '').replace(f'{i + 1}.', '').strip()\n",
        "\n",
        "            # This condition will check if answer has A, B, C ,D or 1,2,3,4 in the beginning then it will remove it\n",
        "            if type(question['answer']) != bool:\n",
        "                question['answer'] = question['answer'].replace('A)', '').replace('a)', '').replace('A.', '').replace('a.', '').replace('1)', '').replace('1.', '').replace('B)', '').replace('b)', '').replace('B.', '').replace('b.', '').replace('2)', '').replace('2.', '').replace('C)', '').replace('c)', '').replace('C.', '').replace('c.', '').replace('3)', '').replace('3.', '').replace('D)', '').replace('d)', '').replace('D.', '').replace('d.', '').replace('4)', '').replace('4.', '').strip()\n",
        "            # question['answer']=answer\n",
        "        return questions_list\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return questions_list"
      ],
      "metadata": {
        "id": "5CvT9mNVIEhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mcq_questions=mcq(final)"
      ],
      "metadata": {
        "id": "72PrqLEgDyF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mcq_questions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__5hAvwnGJqc",
        "outputId": "26ab69bb-b453-439d-f03b-269c7516f575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'question': 'How much money did the postmaster and employees raise for Lencho?',\n",
              "  'options': ['100 pesos', '70 pesos', '50 pesos', '20 pesos'],\n",
              "  'answer': '70 pesos',\n",
              "  'rank': '90',\n",
              "  'context': None,\n",
              "  'page_no': 1,\n",
              "  'statement': None,\n",
              "  'question_type': 'mcq'},\n",
              " {'question': 'Why did Lencho write a letter to God?',\n",
              "  'options': ['He wanted to ask for money',\n",
              "   'He wanted to complain about the hail storm',\n",
              "   'He wanted to ask for a good harvest',\n",
              "   'He wanted to ask for a new tractor'],\n",
              "  'answer': 'He wanted to ask for a good harvest',\n",
              "  'rank': '80',\n",
              "  'context': None,\n",
              "  'page_no': 1,\n",
              "  'statement': None,\n",
              "  'question_type': 'mcq'},\n",
              " {'question': 'What did Lencho do when he received the money from the postmaster?',\n",
              "  'options': ['He was surprised and grateful',\n",
              "   'He was dismayed and angry',\n",
              "   'He did not get surprised and expected it',\n",
              "   'He refused to accept it'],\n",
              "  'answer': 'He was dismayed and angry',\n",
              "  'rank': '70',\n",
              "  'context': None,\n",
              "  'page_no': 1,\n",
              "  'statement': None,\n",
              "  'question_type': 'mcq'},\n",
              " {'question': 'Why did Lencho write another letter to God?',\n",
              "  'options': ['He wanted to ask for more money',\n",
              "   'He wanted to complain about the previous letter',\n",
              "   'He wanted to ask for a new tractor',\n",
              "   'He wanted to ask for a good harvest'],\n",
              "  'answer': 'He wanted to ask for more money',\n",
              "  'rank': '60',\n",
              "  'context': None,\n",
              "  'page_no': 1,\n",
              "  'statement': None,\n",
              "  'question_type': 'mcq'}]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U datasets==2.17.0\n",
        "\n",
        "%pip install --upgrade pip\n",
        "%pip install --disable-pip-version-check \\\n",
        "    torch==1.13.1 \\\n",
        "    torchdata==0.5.1 --quiet\n",
        "\n",
        "%pip install \\\n",
        "    transformers==4.27.2 \\\n",
        "    evaluate==0.4.0 \\\n",
        "    rouge_score==0.1.2 \\\n",
        "    peft==0.3.0 --quiet\n",
        "\n",
        "# Installing the Reinforcement Learning library directly from github.\n",
        "%pip install git+https://github.com/lvwerra/trl.git@25fa1bd"
      ],
      "metadata": {
        "id": "BlMr2RtJ_Q4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, GenerationConfig\n",
        "from datasets import load_dataset\n",
        "from peft import PeftModel, PeftConfig, LoraConfig, TaskType\n",
        "\n",
        "# trl: Transformer Reinforcement Learning library\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
        "from trl import create_reference_model\n",
        "from trl.core import LengthSampler\n",
        "\n",
        "import torch\n",
        "import evaluate\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# tqdm library makes the loops show a smart progress meter.\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "fcNjR3yx_Q81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "id": "HaR6Apvr_RAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
        "!pip install -q datasets bitsandbytes einops wandb"
      ],
      "metadata": {
        "id": "sp-t8g8O_RDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "OiIRP09S_RLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import torch"
      ],
      "metadata": {
        "id": "JwV6mn90_1e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train=load_dataset(\"csv\", data_files=\"/content/drive/MyDrive/AK/Data-Hindi-csv/hindi_train_dataset_2600.csv\",split=\"train\")\n",
        "dataset_val=load_dataset(\"csv\", data_files=\"/content/drive/MyDrive/AK/Data-Hindi-csv/hindi_val_dataset_393.csv\",split=\"train\")\n",
        "\n",
        "# dataset_train = pd.read_excel(\"/content/drive/MyDrive/AK/Hindi/hindi_ds.xlsx\")\n",
        "# dataset_val = pd.read_excel(\"/content/drive/MyDrive/AK/Hindi/hindi_ds_val.xlsx\")"
      ],
      "metadata": {
        "id": "3Id-xLwe_1hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train,dataset_val"
      ],
      "metadata": {
        "id": "yE3g1POx_1ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
        "\n",
        "# # model_name = \"chintan4560/falcon-7b-sharded-bf16\"\n",
        "# # model_name = \"AshishK/AK-openhathi-gptq-4bit\"\n",
        "# model_name = \"shivarama23/OpenHathi-7B-Hi-v0.1-Base-sharded-bf16-1GB\"\n",
        "\n",
        "model_name = \"AshishK/AK-OpenHathi-7B-Hi-Sharded-bf16\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "# model.config.use_cache = False"
      ],
      "metadata": {
        "id": "BhSSX4IVADeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "KdXx3SaFADhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 8\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "r3Gn69kMAKG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "r0jLxn68AMck"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}